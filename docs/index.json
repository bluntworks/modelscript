[
  {
    "kind": "index",
    "content": "# modelscript\n\n[![Coverage Status](https://coveralls.io/repos/github/repetere/modelscript/badge.svg?branch=master)](https://coveralls.io/github/repetere/modelscript?branch=master) [![Build Status](https://travis-ci.org/repetere/modelscript.svg?branch=master)](https://travis-ci.org/repetere/modelscript)\n\n## Description\n\n**modelscript** is a javascript module with simple and efficient tools for data mining and data analysis in JavaScript. **modelscript** can be used with [ML.js](https://github.com/mljs/ml), [pandas-js](https://github.com/StratoDem/pandas-js), and [numjs](https://github.com/numjs/numjs), to approximate the equialent R/Python tool chain in JavaScript.\n\nIn Python, data preperation is typically done in a DataFrame, modelscript encourages a more R like workflow where the data prepration is in it's native structure.\n\n### Installation\n\n```sh\n$ npm i modelscript\n```\n\n### [Full Documentation](https://github.com/repetere/modelscript/blob/master/docs/api.md)\n\n### Usage (basic)\n\n```javascript\n\"modelscript\" : {\n  ml:{ //see https://github.com/mljs/ml\n    UpperConfidenceBound [Class: UpperConfidenceBound]{ // Implementation of the Upper Confidence Bound algorithm\n      predict(), //returns next action based off of the upper confidence bound\n      learn(), //single step trainning method\n      train(), //training method for upper confidence bound calculations\n    },\n    ThompsonSampling [Class: ThompsonSampling]{ //Implementation of the Thompson Sampling algorithm\n      predict(), //returns next action based off of the thompson sampling\n      learn(), //single step trainning method\n      train(), //training method for thompson sampling calculations\n    },\n  },\n  nlp:{ //see https://github.com/NaturalNode/natural\n    ColumnVectorizer [Class: ColumnVectorizer]{ //class creating sparse matrices from a corpus\n      get_tokens(), // Returns a distinct array of all tokens after fit_transform\n      get_vector_array(), //Returns array of arrays of strings for dependent features from sparse matrix word map\n      fit_transform(options), //Fits and transforms data by creating column vectors (a sparse matrix where each row has every word in the corpus as a column and the count of appearances in the corpus)\n      get_limited_features(options), //Returns limited sets of dependent features or all dependent features sorted by word count\n      evaluateString(testString), //returns word map with counts\n      evaluate(testString), //returns new matrix of words with counts in columns\n    }\n  },\n  csv:{\n    loadCSV: [Function: loadCSV], //asynchronously loads CSVs, either a filepath or a remote URI\n    loadTSV: [Function: loadTSV], //asynchronously loads TSVs, either a filepath or a remote URI\n  },\n  model_selection: {\n    train_test_split: [Function: train_test_split], // splits data into training and testing sets\n    cross_validation_split: [Function: kfolds], //splits data into k-folds\n    cross_validate_score: [Function: cross_validate_score],//test model variance and bias\n    grid_search: [Function: grid_search], // tune models with grid search for optimal performance\n  },\n  DataSet [Class: DataSet]: { //class for manipulating an array of objects (typically from CSV data)\n    columnMatrix(vectors), //returns a matrix of values by combining column arrays into a matrix\n    columnArray(columnName, options), // - returns a new array of a selected column from an array of objects, can filter, scale and replace values\n    columnReplace(columnName, options), // - returns a new array of a selected column from an array of objects and replaces empty values, encodes values and scales values\n    columnScale(columnName, options), // - returns a new array of scaled values which can be reverse (descaled). The scaling transformations are stored on the DataSet\n    columnDescale(columnName, options), // - Returns a new array of descaled values\n    selectColumns(columns, options), //returns a list of objects with only selected columns as properties\n    labelEncoder(columnName, options), // - returns a new array and label encodes a selected column\n    labelDecode(columnName, options), // - returns a new array and decodes an encoded column back to the original array values\n    oneHotEncoder(columnName, options), // - returns a new object of one hot encoded values\n    columnMatrix(columnName, options), // - returns a matrix of values from multiple columns\n    columnReducer(newColumnName, options), // - returns a new array of a selected column that is passed a reducer function, this is used to create new columns for aggregate statistics\n    columnMerge(name, data), // - returns a new column that is merged onto the data set\n    filterColumn(options), // - filtered rows of data,\n    fitColumns(options), // - mutates data property of DataSet by replacing multiple columns in a single command\n    static reverseColumnMatrix(options), // returns an array of objects by applying labels to matrix of columns\n    static reverseColumnVector(options), // returns an array of objects by applying labels to column vector\n  },\n  calc:{\n    getTransactions: [Function getTransactions], // Formats an array of transactions into a sparse matrix like format for Apriori/Eclat\n    assocationRuleLearning: [async Function assocationRuleLearning], // returns association rule learning results using apriori\n  },\n  util: {\n    range: [Function], // range helper function\n    rangeRight: [Function], //range right helper function\n    scale: [Function: scale], //scale / normalize data\n    avg: [Function: arithmeticMean], // aritmatic mean\n    mean: [Function: arithmeticMean], // aritmatic mean\n    sum: [Function: sum],\n    max: [Function: max],\n    min: [Function: min],\n    sd: [Function: standardDeviation], // standard deviation\n    StandardScalerTransforms: [Function: StandardScalerTransforms], // returns two functions that can standard scale new inputs and reverse scale new outputs\n    MinMaxScalerTransforms: [Function: MinMaxScalerTransforms], // returns two functions that can mix max scale new inputs and reverse scale new outputs\n    StandardScaler: [Function: StandardScaler], // standardization (z-scores)\n    MinMaxScaler: [Function: MinMaxScaler], // min-max scaling\n    ExpScaler: [Function: ExpScaler], // exponent scaling\n    LogScaler: [Function: LogScaler], // natual log scaling\n    squaredDifference: [Function: squaredDifference], // Returns an array of the squared different of two arrays\n    standardError: [Function: standardError], // The standard error of the estimate is a measure of the accuracy of predictions made with a regression line\n    coefficientOfDetermination: [Function: coefficientOfDetermination],\n    adjustedCoefficentOfDetermination: [Function: adjustedCoefficentOfDetermination],\n    adjustedRSquared: [Function: adjustedCoefficentOfDetermination],\n    rBarSquared: [Function: adjustedCoefficentOfDetermination],\n    r: [Function: coefficientOfCorrelation],\n    coefficientOfCorrelation: [Function: coefficientOfCorrelation],\n    rSquared: [Function: rSquared], //r^2\n    pivotVector: [Function: pivotVector], // returns an array of vectors as an array of arrays\n    pivotArrays: [Function: pivotArrays], // returns a matrix of values by combining arrays into a matrix\n    standardScore: [Function: standardScore], // Calculates the z score of each value in the sample, relative to the sample mean and standard deviation.\n    zScore: [Function: standardScore], // alias for standardScore.\n    approximateZPercentile: [Function: approximateZPercentile], // approximate the p value from a z score\n  },\n  preprocessing: {\n    DataSet: [Class DataSet],\n  },\n}\n```\n\n### Examples (JavaScript / Python / R)\n\n#### Loading CSV Data\n\n##### Javascript\n\n```javascript\nimport { default as jsk } from 'modelscript';\nlet dataset;\n\n//In JavaScript, by default most I/O Operations are asynchronous, see the notes section for more\nms.loadCSV('/some/file/path.csv')\n  .then(csvData=>{\n    dataset = new ms.DataSet(csvData);\n    console.log({csvData});\n    /* csvData [{\n      'Country': 'Brazil',\n      'Age': '44',\n      'Salary': '72000',\n      'Purchased': 'N',\n    },\n    ...\n    {\n      'Country': 'Mexico',\n      'Age': '27',\n      'Salary': '48000',\n      'Purchased': 'Yes',\n    }] */\n  })\n  .catch(console.error);\n\n// or from URL\nms.loadCSV('https://example.com/some/file/path.csv')\n\n```\n\n##### Python\n\n```python\nimport pandas as pd\n\n#Importing the dataset\ndataset = pd.read_csv('/some/file/path.csv')\n```\n\n##### R\n\n```R\n# Importingd the dataset\ndataset = read.csv('Data.csv')\n```\n\n#### Handling Missing Data\n\n##### Javascript\n\n```javascript\n//column Array returns column of data by name\n// [ '44','27','30','38','40','35','','48','50', '37' ]\nconst OringalAgeColumn = dataset.columnArray('Age'); \n\n//column Replace returns new Array with replaced missing data\n//[ '44','27','30','38','40','35',38.77777777777778,'48','50','37' ]\nconst ReplacedAgeMeanColumn = dataset.columnReplace('Age',{strategy:'mean'}); \n\n//fit Columns, mutates dataset\ndataset.fitColumns({\n  columns:[{name:'Age',strategy:'mean'}]\n});\n/*\ndataset\nclass DataSet\n  data:[\n    {\n      'Country': 'Brazil',\n      'Age': '38.77777777777778',\n      'Salary': '72000',\n      'Purchased': 'N',\n    }\n    ...\n  ]\n*/\n```\n\n##### Python\n\n```python\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 3].values\n\n# Taking care of of missing data\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values='NaN', strategy = 'mean', axis=0)\nimputer = imputer.fit(X[:, 1:3])\nX[:, 1:3] = imputer.transform(X[:, 1:3])\n```\n\n##### R\n\n```R\n# Taking care of the missing data\ndataset$Age = ifelse(is.na(dataset$Age),\n                ave(dataset$Age,FUN = function(x) mean(x,na.rm =TRUE)),\n                dataset$Age)\n```\n\n#### One Hot Encoding and Label Encoding\n\n##### Javascript\n\n```javascript\n// [ 'Brazil','Mexico','Ghana','Mexico','Ghana','Brazil','Mexico','Brazil','Ghana', 'Brazil' ]\nconst originalCountry = dataset.columnArray('Country'); \n/*\n{ originalCountry:\n   { Country_Brazil: [ 1, 0, 0, 0, 0, 1, 0, 1, 0, 1 ],\n     Country_Mexico: [ 0, 1, 0, 1, 0, 0, 1, 0, 0, 0 ],\n     Country_Ghana: [ 0, 0, 1, 0, 1, 0, 0, 0, 1, 0 ] },\n    }\n*/\nconst oneHotCountryColumn = dataset.oneHotEncoder('Country');\n\n// [ 'N', 'Yes', 'No', 'f', 'Yes', 'Yes', 'false', 'Yes', 'No', 'Yes' ]\nconst originalPurchasedColumn = dataset.labelEncoder('Purchased');\n// [ 0, 1, 0, 0, 1, 1, 1, 1, 0, 1 ]\nconst encodedBinaryPurchasedColumn = dataset.labelEncoder('Purchased',{ binary:true });\n// [ 0, 1, 2, 3, 1, 1, 4, 1, 2, 1 ]\nconst encodedPurchasedColumn = dataset.labelEncoder('Purchased');\n// [ 'N', 'Yes', 'No', 'f', 'Yes', 'Yes', 'false', 'Yes', 'No', 'Yes' ]\nconst decodedPurchased = dataset.labelDecode('Purchased', { data: encodedPurchasedColumn, });\n\n\n//fit Columns, mutates dataset\ndataset.fitColumns({\n  columns:[\n    {\n      name: 'Purchased',\n      options: {\n        strategy: 'label',\n        labelOptions: {\n          binary: true,\n        },\n      },\n    },\n    {\n      name: 'Country',\n      options: {\n        strategy: 'onehot',\n      },\n    },\n  ]\n});\n```\n\n##### Python\n\n```python\n# Encoding  categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nX[:, 0] = labelencoder_X.fit_transform(X[:, 0])\nonehotencoder = OneHotEncoder(categorical_features=[0])\nX = onehotencoder.fit_transform(X).toarray()\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\n```\n\n##### R\n\n```R\n# Encoding categorical data\ndataset$Country = factor(dataset$Country,\n                         levels = c('Brazil', 'Mexico', 'Ghana'),\n                         labels = c(1, 2, 3))\n\ndataset$Purchased = factor(dataset$Purchased,\n                         levels = c('No', 'Yes'),\n                         labels = c(0, 1))\n```\n\n#### Cross Validation\n\n##### Javascript\n\n```javascript\nconst testArray = [20, 25, 10, 33, 50, 42, 19, 34, 90, 23, ];\n\n// { train: [ 50, 20, 34, 33, 10, 23, 90, 42 ], test: [ 25, 19 ] }\nconst trainTestSplit = ms.cross_validation.train_test_split(testArray,{ test_size:0.2, random_state: 0, });\n\n// [ [ 50, 20, 34, 33, 10 ], [ 23, 90, 42, 19, 25 ] ] \nconst crossValidationArrayKFolds = ms.cross_validation.cross_validation_split(testArray, { folds: 2, random_state: 0, });\n```\n\n##### Python\n\n```python\n#splitting the dataset into trnaing set and test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n```\n\n##### R\n\n```R\n# Splitting the dataset into the training set and test set\nlibrary(caTools)\nset.seed(1)\nsplit = sample.split(dataset$Purchased, SplitRatio = 0.8)\ntraining_set = subset(dataset, split == TRUE)\ntest_set = subset(dataset, split == FALSE)\n```\n\n#### Scaling (z-score / min-mix)\n\n##### Javascript\n\n```javascript\ndataset.columnArray('Salary',{ scale:'standard'}); \ndataset.columnArray('Salary',{ scale:'minmax'}); \n```\n\n##### Python\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n```\n\n### Development\n\n*Make sure you have grunt installed*\n\n```sh\n$ npm i -g grunt-cli jsdoc-to-markdown\n```\n\nFor generating documentation\n```sh\n$ grunt doc\n$ jsdoc2md src/**/*.js  > docs/api.md\n```\n\n### Notes\n\nCheck out [https://github.com/repetere/modelscript](https://github.com/repetere/modelscript) for the full modelscript Documentation\n\n#### A quick word about asynchronous JavaScript\n\nMost machine learning tutorials in Python and R are not using their asynchronous equivalents; however, there is a bias in JavaScript to default to non-blocking operations.\n\nWith the advent of ES7 and Node.js 7+ there are syntax helpers with asynchronous functions. It may be easier to use async/await in JS if you want an approximation close to what a workflow would look like in R/Python\n\n```javascript\nimport * as fs from 'fs-extra';\nimport * as np from 'numjs'; \nimport { default as ml } from 'ml';\nimport { default as pd } from 'pandas-js';\nimport { default as mpn } from 'matplotnode';\nimport { loadCSV, preprocessing } from 'modelscript';\nconst plt = mpn.plot;\n\nvoid async () => {\n  const csvData = await loadCSV('../Data.csv');\n  const rawData = new preprocessing.DataSet(csvData);\n  const fittedData = rawData.fitColumns({\n    columns: [\n      { name: 'Age' },\n      { name: 'Salary' },\n      {\n        name: 'Purchased',\n        options: {\n          strategy: 'label',\n          labelOptions: {\n            binary: true,\n          },\n        }\n      },\n    ]\n  });\n  const dataset = new pd.DataFrame(fittedData);\n  const X = dataset.iloc(\n    [ 0, dataset.length ],\n    [ 0, 3 ]).values;\n  const y = dataset.iloc(\n    [ 0, dataset.length ],\n    3).values;\n  console.log({\n    X,\n    y\n  });\n}();\n\n```\n\n### Testing\n\n```sh\n$ npm i\n$ grunt test\n```\n\n### Contributing\n\nFork, write tests and create a pull request!\n\n### Misc\n\nAs of Node 8, ES modules are still used behind a flag, when running natively as an ES module\n\n```sh\n$ node --experimental-modules my-machine-learning-script.mjs\n# Also there are native bindings that require Python 2.x, make sure if you're using Andaconda, you build with your Python 2.x bin\n$ npm i --python=/usr/bin/python\n ```\n\nLicense\n----\n\nMIT",
    "longname": "/Users/yawjosephetse/Developer/github/repetere/modelscript/README.md",
    "name": "./README.md",
    "static": true,
    "access": "public"
  }
]